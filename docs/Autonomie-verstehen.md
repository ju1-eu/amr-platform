# Theoretisches Konzept: Die vier Ebenen der Autonomie

**Stand:** 2025-12-20 | **Projekt:** AMR Platform | **Status:** Phase 1â€“3 âœ…, Phase 4 ðŸ”œ (URDF/TF/Odom-Bridge/EKF)

Autonomie entsteht nicht â€žin einer Ketteâ€œ, sondern durch parallel laufende Ebenen mit klaren DatenvertrÃ¤gen (Topics/TF) und definierten Sicherheitsgrenzen.

---

## Ebene 1: Reflex und Basis-Sicherheit (Low-Level Control)

**Zweck:** Der Roboter bleibt beherrschbar â€“ auch bei Kommunikationsverlust.

**Ist-Stand (Phase 1):**
Der **ESP32-S3** ist der Echtzeit-Controller:

* Motoransteuerung + Encoder-ISR in **100 Hz** Control-Loop (Core 0)
* Odometrie-Integration intern, Publishing Ã¼ber micro-ROS
* **Failsafe:** Stop, wenn lÃ¤nger als **$2000,\mathrm{ms}$** kein gÃ¼ltiges Kommando ankommt

**Wichtige Klarstellung:**
Ein â€žLiDAR-Notstopp < $0{,}30,\mathrm{m}$â€œ lÃ¤uft aktuell **nicht** auf dem ESP32, weil der LiDAR am Pi hÃ¤ngt. Die Reflex-Ebene ist derzeit primÃ¤r **Comms-Failsafe** + Motor-Stop-Logik.

**Grenze:** Open-Loop/Feedforward (PID aktuell deaktiviert) â†’ keine echte Drehzahlregelung, Richtung bei A-only Encodern heuristisch.

---

## Ebene 2: GedÃ¤chtnis (Frames, Odometrie-Vertrag, Lokalisierung)

**Zweck:** â€žWo bin ich?â€œ â€“ konsistente Pose Ã¼ber TF.

**Ist-Stand (Phase 3):**

* `/scan` ist stabil (RPLidar A1, ca. **$7{,}6,\mathrm{Hz}$**, frame_id: `laser`)
* `/odom_raw` existiert (Pose2D) â€“ aber **noch kein sauberer `/odom` (nav_msgs/Odometry)** + konsistente TF-Kette fÃ¼r SLAM

**NÃ¤chster Schritt (Phase 4):**
Wir bauen den TF-/Odom-Kontrakt, damit SLAM/Nav2 Ã¼berhaupt zuverlÃ¤ssig arbeiten kann:

* Statische Frames aus URDF: `base_footprint â†’ base_link â†’ laser`
* Odom-Bridge: `/odom_raw` â†’ `/odom` + TF `odom â†’ base_footprint`
* Optional danach: EKF (robot_localization), sobald Sensorbasis sinnvoll ist (spÃ¤ter IMU)

**Grenze:** Ohne saubere TF-Kette werden Map/Costmaps â€žinactiveâ€œ oder â€ždriftenâ€œ.

---

## Ebene 3: Strategie (Navigation / Nav2)

**Zweck:** â€žWie komme ich von A nach B?â€œ â€“ global planen, lokal ausweichen.

**Geplante Umsetzung (Phase 6):**

* Nav2 erzeugt `/cmd_vel` aus Karte + Costmaps
* Der Roboter folgt `/cmd_vel` Ã¼ber micro-ROS (Pi â†’ ESP32)

**Voraussetzungen (mÃ¼ssen vorher stabil sein):**

* TF: `map â†’ odom â†’ base_* â†’ laser`
* Topics: `/scan`, `/odom`, `/tf`, `/tf_static`
* Karte: `/map` (aus SLAM oder Map-Server)

**Grenze:** Ohne korrekt kalibrierte Odometrie/Frames wird Nav2 instabil (Fehlpose, falsche Hindernisse, falscher Drehsinn).

---

## Ebene 4: Kognition (Semantik / Vision / AI)

**Zweck:** â€žWas ist das?â€œ â€“ Objekte und Situationen klassifizieren (Mensch/Werkzeug/Hindernis).

**Projektstand:** Optional/spÃ¤ter. (Hailo-8L + Kamera sind aktuell nicht Teil des kritischen Pfades fÃ¼r Phase 4â€“6.)

**Grenze:** Semantik bringt erst Nutzen, wenn Ebene 2â€“3 robust laufen (sonst reagiert das System auf falsche Positionen).

---

## Gesamtbild als Datenfluss

```
Ebene 4 (spÃ¤ter):   Vision/AI (Hailo/Kamera) â”€â”€â–º Semantik/Behavior
Ebene 3 (spÃ¤ter):   Nav2 â”€â”€â–º /cmd_vel
Ebene 2 (Phase 4):  TF + /odom + (spÃ¤ter EKF) â”€â”€â–º SLAM/Map
Ebene 1 (Phase 1):  ESP32 Control + Failsafe â”€â”€â–º Motoren
```

**Kommunikation (Ist):**

| Verbindung         | Technik                           | Wert / Hinweis                  |
| ------------------ | --------------------------------- | ------------------------------- |
| ESP32 â†” Pi 5       | micro-ROS (XRCE-DDS) Ã¼ber USB-CDC | $921600,\mathrm{baud}$          |
| Pi 5 â†” LiDAR       | USB-Serial `/dev/ttyUSB0`         | `/scan` ca. $7{,}6,\mathrm{Hz}$ |
| Pi 5 â†” (AI spÃ¤ter) | PCIe (M.2)                        | optional                        |

---

## Konsequenz fÃ¼r â€žPhase 4 als NÃ¤chstesâ€œ

Phase 4 ist die **Scharnierstelle**: Sie macht aus â€žSensoren liefern Datenâ€œ ein **konsistentes Robot-Modell** (URDF/TF) plus **nutzbare Odometrie** (`/odom`). Erst danach sind SLAM (Phase 5) und Nav2 (Phase 6) technisch sauber aufsetzbar.
